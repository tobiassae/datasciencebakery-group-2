{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 10:37:17.536992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-06 10:37:17.539079: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-06 10:37:17.542254: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-06 10:37:17.551340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736159837.566487    3584 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736159837.570728    3584 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-06 10:37:17.597483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import os\n",
    "import tensorflow\n",
    "import missingno as msno\n",
    "from fancyimpute import IterativeImputer, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 10:37:20.742821: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 58733.8633 - mae: 196.2107 - mean_absolute_percentage_error: 91.8879 - val_loss: 43681.3516 - val_mae: 149.2808 - val_mean_absolute_percentage_error: 58.8203\n",
      "Epoch 2/100\n",
      "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 38759.6602 - mae: 135.7243 - mean_absolute_percentage_error: 53.8050 - val_loss: 25558.0449 - val_mae: 108.4926 - val_mean_absolute_percentage_error: 57.2460\n",
      "Epoch 3/100\n",
      "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 23506.1074 - mae: 110.1181 - mean_absolute_percentage_error: 63.6137 - val_loss: 21844.8906 - val_mae: 110.1299 - val_mean_absolute_percentage_error: 77.8672\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 139\u001b[0m\n\u001b[1;32m    132\u001b[0m model \u001b[38;5;241m=\u001b[39m create_lstm_model(input_shape)\n\u001b[1;32m    134\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    135\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    136\u001b[0m     ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m ]\n\u001b[0;32m--> 139\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    146\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m    149\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:367\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m--> 367\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    369\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/keras/src/callbacks/callback_list.py:147\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    145\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, logs)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    148\u001b[0m     logs \u001b[38;5;241m=\u001b[39m python_utils\u001b[38;5;241m.\u001b[39mpythonify_logs(logs)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "kiwo_url = '../../Data/kiwo.csv'\n",
    "umsatz_url = '../../Data/train.csv'\n",
    "wetter_url = '../../Data/wetter.csv'\n",
    "test_url = '../../Data/test.csv'\n",
    "\n",
    "df_kiwo = pd.read_csv(kiwo_url)\n",
    "df_umsatz = pd.read_csv(umsatz_url)\n",
    "df_wetter = pd.read_csv(wetter_url)\n",
    "df_test = pd.read_csv(test_url)\n",
    "\n",
    "# Prepare datasets\n",
    "df_train = pd.merge(df_umsatz, df_wetter, on='Datum', how='left')\n",
    "df_train = pd.merge(df_train, df_kiwo, on='Datum', how='left')\n",
    "df_test = pd.merge(df_test, df_wetter, on='Datum', how='left')\n",
    "df_test = pd.merge(df_test, df_kiwo, on='Datum', how='left')\n",
    "\n",
    "def prepare_data(data):\n",
    "    data['Datum'] = pd.to_datetime(data['Datum'])\n",
    "    data['DayOfWeek'] = data['Datum'].dt.dayofweek\n",
    "    data['Month'] = data['Datum'].dt.month\n",
    "    \n",
    "    def bin_temperature(row):\n",
    "        month = row['Month']\n",
    "        temperature = row['Temperatur']\n",
    "        \n",
    "        if month in [12, 1, 2]:\n",
    "            if temperature <= 0: return 'Very Cold'\n",
    "            elif temperature <= 5: return 'Cold'\n",
    "            elif temperature <= 10: return 'Mild'\n",
    "            else: return 'Warm'\n",
    "        elif month in [3, 4, 5]:\n",
    "            if temperature <= 10: return 'Cool'\n",
    "            elif temperature <= 15: return 'Mild'\n",
    "            elif temperature <= 25: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "        elif month in [6, 7, 8]:\n",
    "            if temperature <= 15: return 'Cool'\n",
    "            elif temperature <= 20: return 'Mild'\n",
    "            elif temperature <= 30: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "        else:\n",
    "            if temperature <= 10: return 'Cool'\n",
    "            elif temperature <= 15: return 'Mild'\n",
    "            elif temperature <= 25: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "\n",
    "    data['Temperatur_binned'] = data.apply(bin_temperature, axis=1)\n",
    "    data['KielerWoche'] = data['KielerWoche'].fillna(0).astype(int)\n",
    "    \n",
    "    numerical_columns = ['Temperatur', 'Windgeschwindigkeit']\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    data[numerical_columns] = imputer.fit_transform(data[numerical_columns])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['DayOfWeek', 'Month', 'Temperatur_binned', 'KielerWoche']\n",
    "\n",
    "# Prepare datasets\n",
    "df_train = prepare_data(df_train)\n",
    "df_test = prepare_data(df_test)\n",
    "\n",
    "# Split train/validation\n",
    "df_train_shuffled = df_train.sample(frac=1, random_state=42)\n",
    "train_size = int(0.85 * len(df_train))\n",
    "df_train_final = df_train_shuffled.iloc[:train_size].copy()\n",
    "df_val = df_train_shuffled.iloc[train_size:].copy()\n",
    "\n",
    "def prepare_features(data):\n",
    "    features = pd.get_dummies(data[categorical_features], drop_first=False, dtype=int)\n",
    "    features['Windgeschwindigkeit'] = data['Windgeschwindigkeit']\n",
    "    features['Temperatur'] = data['Temperatur']\n",
    "    features['Datum'] = data['Datum']\n",
    "    return features\n",
    "\n",
    "train_features = prepare_features(df_train_final)\n",
    "val_features = prepare_features(df_val)\n",
    "test_features = prepare_features(df_test)\n",
    "\n",
    "train_labels = df_train_final[['Umsatz']]\n",
    "val_labels = df_val[['Umsatz']]\n",
    "\n",
    "def create_sequences(features, labels=None, sequence_length=7):\n",
    "    X = []\n",
    "    y = [] if labels is not None else None\n",
    "    \n",
    "    features_no_date = features.drop('Datum', axis=1)\n",
    "    \n",
    "    for i in range(len(features) - sequence_length + 1):\n",
    "        sequence = features_no_date.iloc[i:i+sequence_length].values\n",
    "        X.append(sequence)\n",
    "        \n",
    "        if labels is not None and i + sequence_length <= len(labels):\n",
    "            y.append(labels.iloc[i+sequence_length-1].values[0])\n",
    "    \n",
    "    return (np.array(X), np.array(y)) if labels is not None else np.array(X)\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train = create_sequences(train_features, train_labels)\n",
    "X_val, y_val = create_sequences(val_features, val_labels)\n",
    "X_test = create_sequences(test_features)\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', tf.keras.metrics.MeanAbsolutePercentageError()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_lstm_model(input_shape)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['mean_absolute_percentage_error'], label='Training MAPE')\n",
    "plt.plot(history.history['val_mean_absolute_percentage_error'], label='Validation MAPE')\n",
    "plt.title('Model MAPE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Datum': df_test['Datum'].iloc[6:],  # Adjust for sequence length\n",
    "    'Predicted_Umsatz': test_predictions.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
