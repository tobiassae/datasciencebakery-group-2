{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 21:55:33.609994: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-05 21:55:33.632706: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-05 21:55:33.807629: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-05 21:55:33.894977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736114134.026031   56603 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736114134.076411   56603 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 21:55:34.544458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import os\n",
    "import tensorflow\n",
    "import missingno as msno\n",
    "from fancyimpute import IterativeImputer, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (7933, 2119)\n",
      "Validation features shape: (1401, 317)\n",
      "Test features shape: (1830, 389)\n",
      "Training labels shape: (7933, 1)\n",
      "Validation labels shape: (1401, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56603/516384390.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_final[col] = df_train_final[col].astype('category')\n",
      "/tmp/ipykernel_56603/516384390.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[col] = df_val[col].astype('category')\n",
      "/tmp/ipykernel_56603/516384390.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_final[col] = df_train_final[col].astype('category')\n",
      "/tmp/ipykernel_56603/516384390.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[col] = df_val[col].astype('category')\n",
      "/tmp/ipykernel_56603/516384390.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_final[col] = df_train_final[col].astype('category')\n",
      "/tmp/ipykernel_56603/516384390.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[col] = df_val[col].astype('category')\n"
     ]
    }
   ],
   "source": [
    "# Load all data files\n",
    "kiwo_url = '../Data/kiwo.csv'\n",
    "umsatz_url = '../Data/train.csv'\n",
    "wetter_url = '../Data/wetter.csv'\n",
    "test_url = '../Data/test.csv'\n",
    "\n",
    "df_kiwo = pd.read_csv(kiwo_url)\n",
    "df_umsatz = pd.read_csv(umsatz_url)\n",
    "df_wetter = pd.read_csv(wetter_url)\n",
    "df_test = pd.read_csv(test_url)\n",
    "\n",
    "# Prepare training data\n",
    "df_train = pd.merge(df_umsatz, df_wetter, on='Datum', how='left')\n",
    "df_train = pd.merge(df_train, df_kiwo, on='Datum', how='left')\n",
    "\n",
    "# Prepare test data\n",
    "df_test = pd.merge(df_test, df_wetter, on='Datum', how='left')\n",
    "df_test = pd.merge(df_test, df_kiwo, on='Datum', how='left')\n",
    "\n",
    "# Apply the same preprocessing to both training and test data\n",
    "def prepare_data(data):\n",
    "    # Convert Datum to datetime\n",
    "    data['Datum'] = pd.to_datetime(data['Datum'])\n",
    "    \n",
    "    # Extract additional features from date\n",
    "    data['DayOfWeek'] = data['Datum'].dt.dayofweek\n",
    "    data['Month'] = data['Datum'].dt.month\n",
    "    \n",
    "    # Create 'temperatur_binned' feature\n",
    "    def bin_temperature(row):\n",
    "        month = row['Month']\n",
    "        temperature = row['Temperatur']\n",
    "        \n",
    "        if month in [12, 1, 2]:  # Winter\n",
    "            if temperature <= 0: return 'Very Cold'\n",
    "            elif temperature <= 5: return 'Cold'\n",
    "            elif temperature <= 10: return 'Mild'\n",
    "            else: return 'Warm'\n",
    "        elif month in [3, 4, 5]:  # Spring\n",
    "            if temperature <= 10: return 'Cool'\n",
    "            elif temperature <= 15: return 'Mild'\n",
    "            elif temperature <= 25: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "        elif month in [6, 7, 8]:  # Summer\n",
    "            if temperature <= 15: return 'Cool'\n",
    "            elif temperature <= 20: return 'Mild'\n",
    "            elif temperature <= 30: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "        else:  # Fall\n",
    "            if temperature <= 10: return 'Cool'\n",
    "            elif temperature <= 15: return 'Mild'\n",
    "            elif temperature <= 25: return 'Warm'\n",
    "            else: return 'Hot'\n",
    "\n",
    "    data['Temperatur_binned'] = data.apply(bin_temperature, axis=1)\n",
    "\n",
    "    # Handle KielerWoche\n",
    "    data['KielerWoche'] = data['KielerWoche'].fillna(0).apply(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    # Handle missing data with KNN imputation\n",
    "    numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    non_numerical_columns = data.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "    imputer_irmi = IterativeImputer()\n",
    "    imputed_numerical = pd.DataFrame(\n",
    "        imputer_irmi.fit_transform(data[numerical_columns]),\n",
    "        columns=numerical_columns,\n",
    "        index=data.index\n",
    "    )\n",
    "\n",
    "    data = pd.concat([imputed_numerical, data[non_numerical_columns]], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Prepare both datasets\n",
    "df_train = prepare_data(df_train)\n",
    "df_test = prepare_data(df_test)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.85 * len(df_train))  # Using 85% for training, 15% for validation\n",
    "df_train_final = df_train[:train_size]\n",
    "df_val = df_train[train_size:]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['Warengruppe', 'Temperatur_binned', 'DayOfWeek', 'KielerWoche', 'Month', 'Wettercode']\n",
    "\n",
    "# Process categorical features\n",
    "for col in categorical_features:\n",
    "    df_train_final[col] = df_train_final[col].astype('category')\n",
    "    df_val[col] = df_val[col].astype('category')\n",
    "    df_test[col] = df_test[col].astype('category')\n",
    "\n",
    "# Create feature sets\n",
    "def prepare_features(data):\n",
    "    features = pd.get_dummies(data[categorical_features], drop_first=False, dtype=int)\n",
    "    features['Windgeschwindigkeit'] = data['Windgeschwindigkeit']\n",
    "    features['Datum'] = data['Datum']\n",
    "    return features\n",
    "\n",
    "# Prepare features for all datasets\n",
    "train_features = prepare_features(df_train_final)\n",
    "val_features = prepare_features(df_val)\n",
    "test_features = prepare_features(df_test)\n",
    "\n",
    "# Prepare labels (only for train and validation)\n",
    "train_labels = df_train_final[['Umsatz']]\n",
    "val_labels = df_val[['Umsatz']]\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Training features shape:\", train_features.shape)\n",
    "print(\"Validation features shape:\", val_features.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Create sequences from your existing splits\u001b[39;00m\n\u001b[1;32m     53\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# Week of historical data\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m create_sequences(\u001b[43mtraining_features\u001b[49m, training_labels, sequence_length)\n\u001b[1;32m     55\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m create_sequences(validation_features, validation_labels, sequence_length)\n\u001b[1;32m     56\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m create_sequences(test_features, test_labels, sequence_length)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_features' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "# Custom MAPE metric\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def create_sequences(features, labels, sequence_length=7):\n",
    "    \"\"\"Create sequences for LSTM input\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Remove 'Datum' column for sequence creation\n",
    "    features_no_date = features.drop('Datum', axis=1)\n",
    "    \n",
    "    for i in range(len(features) - sequence_length + 1):\n",
    "        sequence = features_no_date.iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # Check if there is a valid label available for this sequence\n",
    "        if i + sequence_length <= len(labels):\n",
    "            X.append(sequence)\n",
    "            y.append(labels.iloc[i+sequence_length-1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    \"\"\"Create LSTM model with the specified input shape\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', mean_absolute_percentage_error]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create sequences from your existing splits\n",
    "sequence_length = 7  # Week of historical data\n",
    "X_train, y_train = create_sequences(training_features, training_labels, sequence_length)\n",
    "X_val, y_val = create_sequences(validation_features, validation_labels, sequence_length)\n",
    "X_test, y_test = create_sequences(test_features, test_labels, sequence_length)\n",
    "\n",
    "# Get input shape from the processed data\n",
    "n_features = X_train.shape[2]  # Number of features after one-hot encoding\n",
    "input_shape = (X_train.shape[1], n_features)  # Sequence length and number of features\n",
    "\n",
    "# Create and train the model\n",
    "model = create_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae, test_mape = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nTest Results:')\n",
    "print(f'MAE: {test_mae:.2f}')\n",
    "print(f'MAPE: {test_mape:.2f}%')\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "val_predictions = model.predict(X_val)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Plotting training history\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# MAE plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "# MAPE plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['mean_absolute_percentage_error'], label='Training MAPE')\n",
    "plt.plot(history.history['val_mean_absolute_percentage_error'], label='Validation MAPE')\n",
    "plt.title('Model MAPE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print detailed metrics for each dataset\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "print_metrics(y_train, train_predictions, \"Training\")\n",
    "print_metrics(y_val, val_predictions, \"Validation\")\n",
    "print_metrics(y_test, test_predictions, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
